# A main limitation of model training is stability.
# To address this issue we use
# - a small learning rate of 0.0001 to account for the small batch size
# - gradient clipping to solve the exploding gradient problem
# - lower weights for the loss function
# Adding noise with standard deviation of 1e-10 before computing SVD for improved
# numerical stability

# Model parameters
num_unroll_blocks: 10
num_res_blocks: 15
CG_Iter: 10
activation: "ReLU"
features: 32
groups: 171
phase_normalization: true

# Loss function parameters
weight_kspace_loss: [0.5, 0.0]
weight_ispace_loss: [0.0, 0.0]
weight_wavelet_loss: [0.0, 0.0]
weight_hankel_loss: [0.0, 0.0]
weight_casorati_loss: [0.0, 0.0]
dim_kspace_loss:
dim_ispace_loss:
dim_wavelet_loss:
normalized_loss: true

# Training parameters
epochs: 25
optimizer: "Adam"
lr: 0.0001
betas: [0.9, 0.999]
eps: 1.0e-8
weight_decay: 0.0
averaged_model: "Lookahead"
lookahead_alpha: 0.5
lookahead_k: 5
normalized_gradient: false
model_training: true
model_validation: false
fractions: [0.0, 0.5, 0.5]

save_checkpoint: true
checkpoint_frequency: 10
single_epoch: true

load_model_state: true
load_averaged_model_state: true
load_optim_state: true
load_metrics: true

disable_progress_bar: true
timing_level: 0
validation_level: 0

num_threads: 24
num_cpu_per_worker: 24
num_gpu_per_worker: 1

num_workers: 2
group_size: 2
batch_size: 1
use_gpu: true
device: "cuda"

data_dir: "qrage/sessions/%s/preproc.zarr/preproc.zarr"
data_backend: "local"
session_dir: "qrage/sessions/%s/preproc.zarr/preproc.zarr"
session_backend: "local"
model_dir: "qrage/models/ssl_512_features_ddp/hankel_dual_domain_v34"
model_backend: "local"
image_dir: "qrage/recons/%s/images-hankel-dual-domain-v34%s.zarr/images.zarr"
image_backend: "local"
root_dir: "/p/home/jusers/zimmermann9/jureca/drecon"
endpoint_url: "https://s3.fz-juelich.de"

datasets: ["7T1259", "7T1266", "7T1270", "7T1276"]
slices:
  start: 0
  stop: 160
  step: 1
shape: [256, 256, 19, 9]
num_spokes: 8
model_tag: ""
