{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "effa8bdf-2362-46c4-9000-00e62e3695a3",
   "metadata": {},
   "source": [
    "# Dual-Domain-Training\n",
    "## Import needed Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18ac32d3-5ed3-4ae9-a216-bba86b4006e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../src\")\n",
    "\n",
    "import gc\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "from juart.dl.checkpoint.manager import CheckpointManager\n",
    "from juart.dl.data.training import DatasetTraining\n",
    "from juart.dl.loss.loss import JointLoss\n",
    "from juart.dl.model.unrollnet import (\n",
    "    ExponentialMovingAverageModel,\n",
    "    LookaheadModel,\n",
    "    SingleContrastUnrolledNet,\n",
    "    UnrolledNet,\n",
    ")\n",
    "from juart.dl.operation.modules import training, validation\n",
    "from juart.dl.utils.dist import GradientAccumulator\n",
    "\n",
    "import os\n",
    "\n",
    "if os.getenv(\"ZS_SSL_RECON_SOFTWARE_DIR\") is not None:\n",
    "    sys.path.insert(0, os.getenv(\"ZS_SSL_RECON_SOFTWARE_DIR\"))\n",
    "\n",
    "from ray.train import ScalingConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "from juart.dl.utils.parser import options_parser\n",
    "from juart.dl.train.train import train_loop_per_worker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9a2a28-d19c-4596-b69a-b23ee24ec692",
   "metadata": {},
   "source": [
    "## Define all necessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a088bb8b-e09d-489a-ac84-a5d6ba185d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "num_unroll_blocks = 10\n",
    "num_res_blocks = 15\n",
    "CG_Iter = 10\n",
    "activation = \"ReLU\"\n",
    "features = 512\n",
    "directory = (\n",
    "    \"model_test_checkpoint2\"  # Name that is used for the save directory of the model\n",
    ")\n",
    "root_dir = \"/home/jovyan/models\"  # path of the model directory\n",
    "backend = \"local\"  # backend of the model directory\n",
    "\n",
    "# Loss function parameters\n",
    "weight_kspace_loss = [0.5, 0.5]\n",
    "weight_ispace_loss = [0.1, 0.1]\n",
    "weight_hankel_loss = [0.0, 0.01]\n",
    "weight_casorati_loss = [0.0, 0.0]\n",
    "weight_wavelet_loss = [0.0, 0.0]\n",
    "normalized_loss = True\n",
    "\n",
    "# Training parameters\n",
    "epochs = 25\n",
    "model_training = True\n",
    "model_validation = False\n",
    "ema_decay = 0.9\n",
    "fractions = [0.0, 0.5, 0.5]\n",
    "\n",
    "optimizer = \"Adam\"\n",
    "normalized_gradient = False\n",
    "\n",
    "averaged_model = \"Lookahead\"\n",
    "\n",
    "save_checkpoint = True\n",
    "checkpoint_frequency = 10\n",
    "\n",
    "load_model_state = True\n",
    "load_averaged_model_state = True\n",
    "load_optim_state = True\n",
    "load_metrics = True\n",
    "\n",
    "disable_progress_bar = True\n",
    "timing_level = 0\n",
    "validation_level = 0\n",
    "\n",
    "num_threads = 24\n",
    "num_cpu_per_worker = 24\n",
    "num_gpu_per_worker = 0\n",
    "num_workers = 1\n",
    "group_size = 1\n",
    "use_gpu = True\n",
    "device = \"cuda:3\"\n",
    "\n",
    "data_dir = \"\"\n",
    "data_backend = \"local\"\n",
    "model_dir = \"\"\n",
    "model_backend = \"local\"\n",
    "image_dir = \"\"\n",
    "image_backend = \"local\"\n",
    "endpoint_url = \"https://s3.fz-juelich.de\"\n",
    "\n",
    "datasets = []\n",
    "slices = []\n",
    "start = 0\n",
    "stop = 3\n",
    "step = 1\n",
    "shape = 256, 256, 256, 2, 2\n",
    "num_spokes = 8\n",
    "batch_size = 1\n",
    "groups = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd94c5d3-b665-40c9-85a4-26a887b2728a",
   "metadata": {},
   "source": [
    "## Initializing process group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bffbe364-b525-40c2-9dec-78219148aa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.init_process_group(\n",
    "    backend=\"gloo\", init_method=\"tcp://127.0.0.1:23456\", world_size=1, rank=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5081978-20ea-4a8c-af07-9eded7c2c994",
   "metadata": {},
   "source": [
    "## Trainloop_per_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a56a4101-65d5-4b76-ba0e-ec48be9e0887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0 - Intialize local groups ...\n",
      "Rank 0 is in group [0] ...\n",
      "Rank 0 is local rank 0 ...\n",
      "Rank 0 - Using CUDA device 0 of 4 ...\n",
      "Rank 0 is using device cuda:0 ...\n",
      "Rank 0 - LookaheadModel\n",
      "Rank 0 - Loading model state ...\n",
      "Rank 0 - Could not load model state.\n",
      "Rank 0 - Loading averaged model state ...\n",
      "Rank 0 - Could not load averaged model state.\n",
      "Rank 0 - Loading optim state ...\n",
      "Rank 0 - Could not load optim state.\n",
      "Rank 0 - Loading metrics ...\n",
      "Rank 0 - Could not load metrics.\n",
      "Rank 0 - Continue with iteration 0 ...\n"
     ]
    }
   ],
   "source": [
    "def shuffled_indices(num_samples, num_epochs, rng):\n",
    "    indices = np.repeat(np.arange(num_samples), num_epochs)\n",
    "    indices = indices.reshape((num_samples, num_epochs))\n",
    "    indices = rng.permuted(indices, axis=0)\n",
    "    indices = indices.T.ravel()\n",
    "\n",
    "    # Check if each sample is used once and only once in every epoch\n",
    "    assert indices.size == num_samples * num_epochs\n",
    "    for i in np.split(indices, num_epochs):\n",
    "        assert np.unique(i).size == num_samples\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "torch.set_num_threads(num_threads)\n",
    "#torch.set_num_interop_threads(num_threads)\n",
    "\n",
    "global_rank = int(dist.get_rank())\n",
    "world_size = int(dist.get_world_size())\n",
    "\n",
    "print(f\"Rank {global_rank} - Intialize local groups ...\")\n",
    "dist.barrier()\n",
    "\n",
    "dist.barrier()\n",
    "\n",
    "for rank in range(0, world_size, group_size):\n",
    "    ranks = list(range(rank, rank + group_size, 1))\n",
    "    if global_rank in ranks:\n",
    "        print(f\"Rank {global_rank} is in group {ranks} ...\")\n",
    "        group = dist.new_group(ranks, backend=\"gloo\")\n",
    "    dist.barrier()\n",
    "\n",
    "group_rank = dist.get_group_rank(group, global_rank)\n",
    "group_index = global_rank // group_size\n",
    "num_groups = world_size // group_size\n",
    "\n",
    "print(f\"Rank {global_rank} is local rank {group_rank} ...\")\n",
    "\n",
    "dist.barrier()\n",
    "\n",
    "if use_gpu and torch.cuda.is_available():\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    device_rank = np.mod(global_rank, torch.cuda.device_count())\n",
    "    device = f\"cuda:{device_rank}\"\n",
    "    print(\n",
    "        f\"Rank {global_rank} - Using CUDA device {device_rank} of {num_devices} ...\"\n",
    "    )\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Rank {global_rank} is using device {device} ...\")\n",
    "\n",
    "dist.barrier()\n",
    "\n",
    "nD = len(datasets)\n",
    "nS = len(slices)\n",
    "nX, nY, nZ, nTI, nTE = shape\n",
    "\n",
    "num_epochs = epochs\n",
    "\n",
    "# The number of batches that are computed serially via gradient accumulation\n",
    "batch_size = batch_size\n",
    "batch_size_local = batch_size // num_groups\n",
    "\n",
    "num_iterations = nD * nS * num_epochs\n",
    "\n",
    "rng = np.random.default_rng(seed=0)\n",
    "\n",
    "training_indices = shuffled_indices(nD * nS, num_epochs, rng)\n",
    "training_indices_batched = training_indices.reshape(\n",
    "    (-1, batch_size_local, num_groups)\n",
    ")\n",
    "\n",
    "validation_indices = shuffled_indices(nD * nS, num_epochs, rng)\n",
    "validation_indices_batched = validation_indices.reshape(\n",
    "    (-1, batch_size_local, num_groups)\n",
    ")\n",
    "\n",
    "# Prepare models and optimizer\n",
    "\n",
    "if groups == 1:\n",
    "    model = UnrolledNet(\n",
    "        shape,\n",
    "        features=features,\n",
    "        CG_Iter=CG_Iter,\n",
    "        num_unroll_blocks=num_unroll_blocks,\n",
    "        # weight_standardization=options[\"weight_standardization\"],\n",
    "        # spectral_normalization=options[\"spectral_normalization\"],\n",
    "        activation=activation,\n",
    "        disable_progress_bar=disable_progress_bar,\n",
    "        timing_level=timing_level,\n",
    "        validation_level=validation_level,\n",
    "        device=device,\n",
    "    )\n",
    "else:\n",
    "    model = SingleContrastUnrolledNet(\n",
    "        shape,\n",
    "        features=features,\n",
    "        CG_Iter=CG_Iter,\n",
    "        num_unroll_blocks=num_unroll_blocks,\n",
    "        # weight_standardization=options[\"weight_standardization\"],\n",
    "        # spectral_normalization=options[\"spectral_normalization\"],\n",
    "        activation=activation,\n",
    "        disable_progress_bar=disable_progress_bar,\n",
    "        timing_level=timing_level,\n",
    "        validation_level=validation_level,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "loss_fn = JointLoss(\n",
    "    shape,\n",
    "    (3, 3),\n",
    "    weights_kspace_loss=weight_kspace_loss,\n",
    "    weights_ispace_loss=weight_ispace_loss,\n",
    "    weights_wavelet_loss=weight_wavelet_loss,\n",
    "    weights_hankel_loss=weight_hankel_loss,\n",
    "    weights_casorati_loss=weight_casorati_loss,\n",
    "    normalized_loss=normalized_loss,\n",
    "    timing_level=timing_level,\n",
    "    validation_level=validation_level,\n",
    "    group=group,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "if optimizer == \"Adam\":\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=0.0001,\n",
    "        betas=[0.9, 0.999],\n",
    "        eps=1.0e-8,\n",
    "        weight_decay=0.0,\n",
    "    )\n",
    "elif optimizer == \"AdamW\":\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=0.0001,\n",
    "        betas=[0.9, 0.999],\n",
    "        eps=1.0e-8,\n",
    "        weight_decay=0.0,\n",
    "    )\n",
    "elif optimizer == \"RAdam\":\n",
    "    optimizer = torch.optim.RAdam(\n",
    "        model.parameters(),\n",
    "        lr=0.0001,\n",
    "        betas=[0.9, 0.999],\n",
    "        eps=1.0e-8,\n",
    "        weight_decay=0.0,\n",
    "    )\n",
    "\n",
    "accumulator = GradientAccumulator(\n",
    "    model,\n",
    "    accumulation_steps=batch_size_local,\n",
    "    max_norm=1.0,\n",
    "    normalized_gradient=normalized_gradient\n",
    ")\n",
    "\n",
    "if averaged_model == \"EMA\":\n",
    "    print(f\"Rank {global_rank} - ExponentialMovingAverageModel\")\n",
    "    averaged_model = ExponentialMovingAverageModel(\n",
    "        model,\n",
    "        decay=ema_decay,\n",
    "    )\n",
    "elif averaged_model == \"Lookahead\":\n",
    "    print(f\"Rank {global_rank} - LookaheadModel\")\n",
    "    averaged_model = LookaheadModel(\n",
    "        model,\n",
    "        alpha=0.5,\n",
    "        k=5,\n",
    "    )\n",
    "\n",
    "checkpoint_manager = CheckpointManager(\n",
    "    model_dir,\n",
    "    root_dir=root_dir,\n",
    "    endpoint_url=endpoint_url,\n",
    "    backend=model_backend,\n",
    ")\n",
    "\n",
    "if load_model_state:\n",
    "    print(f\"Rank {global_rank} - Loading model state ...\")\n",
    "    checkpoint = checkpoint_manager.load([\"model_state\"], map_location=device)\n",
    "    if all(checkpoint.values()):\n",
    "        model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    else:\n",
    "        print(f\"Rank {global_rank} - Could not load model state.\")\n",
    "\n",
    "if load_averaged_model_state:\n",
    "    print(f\"Rank {global_rank} - Loading averaged model state ...\")\n",
    "    checkpoint = checkpoint_manager.load(\n",
    "        [\"averaged_model_state\"], map_location=device\n",
    "    )\n",
    "    if all(checkpoint.values()):\n",
    "        averaged_model.load_state_dict(checkpoint[\"averaged_model_state\"])\n",
    "    else:\n",
    "        print(f\"Rank {global_rank} - Could not load averaged model state.\")\n",
    "\n",
    "if load_optim_state:\n",
    "    print(f\"Rank {global_rank} - Loading optim state ...\")\n",
    "    checkpoint = checkpoint_manager.load([\"optim_state\"], map_location=device)\n",
    "    if all(checkpoint.values()):\n",
    "        optimizer.load_state_dict(checkpoint[\"optim_state\"])\n",
    "    else:\n",
    "        print(f\"Rank {global_rank} - Could not load optim state.\")\n",
    "\n",
    "    total_trn_loss = list()\n",
    "    total_val_loss = list()\n",
    "    iteration = 0\n",
    "\n",
    "if load_metrics:\n",
    "    print(f\"Rank {global_rank} - Loading metrics ...\")\n",
    "    checkpoint = checkpoint_manager.load([\"trn_loss\", \"val_loss\", \"iteration\"])\n",
    "    if all(checkpoint.values()):\n",
    "        total_trn_loss = list(checkpoint[\"trn_loss\"])\n",
    "        total_val_loss = list(checkpoint[\"val_loss\"])\n",
    "        iteration = checkpoint[\"iteration\"]\n",
    "    else:\n",
    "        print(f\"Rank {global_rank} - Could not load metrics.\")\n",
    "\n",
    "print(f\"Rank {global_rank} - Continue with iteration {iteration} ...\")\n",
    "\n",
    "training_data = DatasetTraining(\n",
    "    data_dir,\n",
    "    datasets,\n",
    "    slices,\n",
    "    num_spokes,\n",
    "    fractions,\n",
    "    mode=\"training\",\n",
    "    group_rank=group_rank,\n",
    "    root_dir=root_dir,\n",
    "    endpoint_url=endpoint_url,\n",
    "    backend=data_backend,\n",
    ")\n",
    "\n",
    "while iteration < num_iterations:\n",
    "    tic = time.time()\n",
    "\n",
    "    # Reset the seed so that training can be resumed\n",
    "    np.random.seed(iteration)\n",
    "    torch.manual_seed(iteration)\n",
    "\n",
    "    training_index = training_indices_batched[\n",
    "        iteration // batch_size,\n",
    "        :,\n",
    "        group_index,\n",
    "    ].tolist()\n",
    "    validation_index = validation_indices_batched[\n",
    "        iteration // batch_size, :, group_index\n",
    "    ].tolist()\n",
    "\n",
    "    if options[\"model_training\"]:\n",
    "        print(f\"Rank {global_rank} - Training index {training_index} ...\")\n",
    "\n",
    "        trn_loss = training(\n",
    "            training_index,\n",
    "            training_data,\n",
    "            model,\n",
    "            loss_fn,\n",
    "            optimizer,\n",
    "            accumulator,\n",
    "            group=group,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        averaged_model.update_parameters(\n",
    "            model,\n",
    "        )\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    else:\n",
    "        trn_loss = [0] * batch_size\n",
    "\n",
    "    if options[\"model_validation\"]:\n",
    "        print(f\"Rank {global_rank} - Validation index {validation_index} ...\")\n",
    "\n",
    "        val_loss = validation(\n",
    "            validation_index,\n",
    "            validation_data,\n",
    "            averaged_model,\n",
    "            loss_fn,\n",
    "            group=group,\n",
    "            device=device,\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    else:\n",
    "        val_loss = [0] * batch_size\n",
    "\n",
    "    total_trn_loss += trn_loss\n",
    "    total_val_loss += val_loss\n",
    "\n",
    "    if global_rank == 0:\n",
    "        # Completed epoch\n",
    "        if (\n",
    "            options[\"save_checkpoint\"]\n",
    "            and np.mod(iteration + batch_size, nD * nS) == 0\n",
    "        ):\n",
    "            print(\"Creating tagged checkpoint ...\")\n",
    "\n",
    "            checkpoint = {\n",
    "                \"iteration\": iteration + batch_size,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"averaged_model_state\": averaged_model.state_dict(),\n",
    "                \"optim_state\": optimizer.state_dict(),\n",
    "                \"trn_loss\": total_trn_loss,\n",
    "                \"val_loss\": total_val_loss,\n",
    "            }\n",
    "\n",
    "            epoch = (iteration + batch_size) // (nD * nS)\n",
    "            checkpoint_manager.save(checkpoint, tag=f\"_epoch_{epoch}\")\n",
    "\n",
    "            if options[\"single_epoch\"]:\n",
    "                # Also save the checkpoint as untagged checkpoint\n",
    "                # Otherwise, training will be stuck in endless loop\n",
    "                checkpoint_manager.save(checkpoint)\n",
    "                checkpoint_manager.release()\n",
    "                break\n",
    "\n",
    "        # Intermediate checkpoint\n",
    "        elif (\n",
    "            options[\"save_checkpoint\"]\n",
    "            and np.mod(iteration + batch_size, options[\"checkpoint_frequency\"]) == 0\n",
    "        ):\n",
    "            print(\"Creating untagged checkpoint ...\")\n",
    "\n",
    "            checkpoint = {\n",
    "                \"iteration\": iteration + batch_size,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"averaged_model_state\": averaged_model.state_dict(),\n",
    "                \"optim_state\": optimizer.state_dict(),\n",
    "                \"trn_loss\": total_trn_loss,\n",
    "                \"val_loss\": total_val_loss,\n",
    "            }\n",
    "\n",
    "            checkpoint_manager.save(checkpoint, block=False)\n",
    "\n",
    "        toc = time.time() - tic\n",
    "\n",
    "        print(\n",
    "            (\n",
    "                f\"Iteration: {iteration} - \"\n",
    "                + f\"Elapsed time: {toc:.0f} - \"\n",
    "                + f\"Training loss: {[f'{loss:.3f}' for loss in trn_loss]} - \"\n",
    "                + f\"Validation loss: {[f'{loss:.3f}' for loss in val_loss]}\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    iteration += batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fce1c8-40e4-442b-8476-28e7002743cb",
   "metadata": {},
   "source": [
    "## Ray-Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "307b8e82-5682-43a3-a8d9-aa449599b1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.13/site-packages/ray/train/base_trainer.py:566: RayDeprecationWarning: `ray.train.ScalingConfig(trainer_resources)` is deprecated. This parameter was an advanced configuration that specified resources for the Ray Train driver actor, which doesn't need to reserve logical resources because it doesn't perform any heavy computation. Only the `resources_per_worker` parameter should be used to specify resources for the training workers. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "  _log_deprecation_warning(TRAINER_RESOURCES_DEPRECATION_MESSAGE)\n",
      "2025-09-12 13:54:23,389\tWARNING services.py:2148 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67092480 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2025-09-12 13:54:24,662\tINFO worker.py:1942 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-09-12 13:54:29,074\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `<FrameworkTrainer>(...)`.\n",
      "2025-09-12 13:54:29,076\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2025-09-12 13:54:29,081\tINFO tensorboardx.py:193 -- pip install \"ray[tune]\" to see TensorBoard files.\n",
      "2025-09-12 13:54:29,082\tWARNING callback.py:143 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-09-12 13:54:29 (running for 00:00:00.12)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/128 CPUs, 0/4 GPUs (0.0/1.0 accelerator_type:A100)\n",
      "Result logdir: /tmp/ray/session_2025-09-12_13-54-19_022763_1719359/artifacts/2025-09-12_13-54-29/TorchTrainer_2025-09-12_13-54-19/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "\u001b[36m(autoscaler +19s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "\u001b[33m(autoscaler +19s)\u001b[0m Error: No available node types can fulfill resource request {'CPU': 24.0, 'GPU': 26.0}. Add suitable node types to this cluster to resolve this issue.\n",
      "== Status ==\n",
      "Current time: 2025-09-12 13:54:34 (running for 00:00:05.14)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/128 CPUs, 0/4 GPUs (0.0/1.0 accelerator_type:A100)\n",
      "Result logdir: /tmp/ray/session_2025-09-12_13-54-19_022763_1719359/artifacts/2025-09-12_13-54-29/TorchTrainer_2025-09-12_13-54-19/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-09-12 13:54:39 (running for 00:00:10.17)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/128 CPUs, 0/4 GPUs (0.0/1.0 accelerator_type:A100)\n",
      "Result logdir: /tmp/ray/session_2025-09-12_13-54-19_022763_1719359/artifacts/2025-09-12_13-54-29/TorchTrainer_2025-09-12_13-54-19/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-09-12 13:54:44 (running for 00:00:15.19)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/128 CPUs, 0/4 GPUs (0.0/1.0 accelerator_type:A100)\n",
      "Result logdir: /tmp/ray/session_2025-09-12_13-54-19_022763_1719359/artifacts/2025-09-12_13-54-29/TorchTrainer_2025-09-12_13-54-19/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-09-12 13:54:49 (running for 00:00:20.21)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/128 CPUs, 0/4 GPUs (0.0/1.0 accelerator_type:A100)\n",
      "Result logdir: /tmp/ray/session_2025-09-12_13-54-19_022763_1719359/artifacts/2025-09-12_13-54-29/TorchTrainer_2025-09-12_13-54-19/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-09-12 13:54:54 (running for 00:00:25.23)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/128 CPUs, 0/4 GPUs (0.0/1.0 accelerator_type:A100)\n",
      "Result logdir: /tmp/ray/session_2025-09-12_13-54-19_022763_1719359/artifacts/2025-09-12_13-54-29/TorchTrainer_2025-09-12_13-54-19/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnProcess-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.13/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/opt/conda/lib/python3.13/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/juart/examples/dl/../../src/juart/dl/checkpoint/manager.py\", line 89, in save_checkpoint_process\n",
      "    self.save_buffer_to_filesystem(*self.save_queue.get())\n",
      "                                    ~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/opt/conda/lib/python3.13/multiprocessing/queues.py\", line 101, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/opt/conda/lib/python3.13/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/conda/lib/python3.13/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/conda/lib/python3.13/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "2025-09-12 13:54:56,331\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2025-09-12 13:54:56,336\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/jovyan/ray_results/TorchTrainer_2025-09-12_13-54-19' in 0.0037s.\n",
      "2025-09-12 13:54:56,338\tINFO tune.py:1041 -- Total run time: 27.26 seconds (27.24 seconds for the tuning loop).\n",
      "2025-09-12 13:54:56,339\tWARNING tune.py:1051 -- Training has been interrupted, but the most recent state was saved.\n",
      "Resume training with: <FrameworkTrainer>.restore(path=\"/home/jovyan/ray_results/TorchTrainer_2025-09-12_13-54-19\", ...)\n",
      "2025-09-12 13:54:56,344\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 1 trial(s):\n",
      "- TorchTrainer_0068e_00000: FileNotFoundError('Could not fetch metrics for TorchTrainer_0068e_00000: both result.json and progress.csv were not found at /home/jovyan/ray_results/TorchTrainer_2025-09-12_13-54-19/TorchTrainer_0068e_00000_0_2025-09-12_13-54-29')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-09-12 13:54:56 (running for 00:00:27.25)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/128 CPUs, 0/4 GPUs (0.0/1.0 accelerator_type:A100)\n",
      "Result logdir: /tmp/ray/session_2025-09-12_13-54-19_022763_1719359/artifacts/2025-09-12_13-54-29/TorchTrainer_2025-09-12_13-54-19/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    #options = options_parser()\n",
    "\n",
    "    scaling_config = ScalingConfig(\n",
    "        num_workers=num_workers,\n",
    "        trainer_resources={\"CPU\": 0, \"GPU\": 2},\n",
    "        resources_per_worker={\n",
    "            \"CPU\": num_cpu_per_worker,\n",
    "            \"GPU\": num_cpu_per_worker,\n",
    "        },\n",
    "        use_gpu=use_gpu,\n",
    "    )\n",
    "\n",
    "    # Initialize the Trainer.\n",
    "    trainer = TorchTrainer(\n",
    "        train_loop_per_worker=train_loop_per_worker,\n",
    "        #train_loop_config=options,\n",
    "        scaling_config=scaling_config,\n",
    "    )\n",
    "\n",
    "    # Train the model.\n",
    "    trainer.fit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24931380-78f6-466b-aee8-4236f5594cca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
