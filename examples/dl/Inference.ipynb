{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of all needed scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../src\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from juart.dl.checkpoint.manager import CheckpointManager\n",
    "from juart.dl.data.inference import DatasetInference\n",
    "from juart.dl.model.dc import DataConsistency\n",
    "from juart.dl.model.resnet import ResNet\n",
    "from juart.dl.model.unrollnet import ExponentialMovingAverageModel, UnrolledNet\n",
    "from juart.dl.operation.modules import inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To improve performance, manually limit the number of threads\n",
    "# torch.set_num_threads(16)\n",
    "# torch.set_num_interop_threads(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define important variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset options\n",
    "i_slice = [80]  # number of slice that will be reconstructed\n",
    "num_spokes = 64  # number of spokes that are used for the reconstruction\n",
    "\n",
    "# device options\n",
    "device = \"cpu\"  # device on which the reconstructions will run\n",
    "\n",
    "# CheckpointManager options\n",
    "directory = \"model_64spokes_1epoch\"  # directory of the dl-qrage model\n",
    "root_dir = \"/home/jovyan/models\"  # the path to the model directory\n",
    "backend = \"local\"  # the backend of the model directory\n",
    "\n",
    "# model options\n",
    "nX, nY, nZ, nTI, nTE = (\n",
    "    256,\n",
    "    256,\n",
    "    1,\n",
    "    2,\n",
    "    2,\n",
    ")  # number of pixels in every direction // number of measurements at T1/T2 decay\n",
    "shape = (\n",
    "    nX,\n",
    "    nY,\n",
    "    nZ,\n",
    "    nTI,\n",
    "    nTE,\n",
    ")  # ordered structure of the parameters above that will be passed to the model\n",
    "features = 64  # number of hidden_inputs of the dl-qrage model\n",
    "cg_iter = 10  # number of cg iterations in the dl-qrage model reconstruction\n",
    "\n",
    "# display options\n",
    "vmax = 2  # sets the brightness normalization of the display between 0 and vmax\n",
    "iTI, iTE = 1, 0  # sets the number of measurement that should be displayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "The DatasetInference is used to load a Dataset which is ready for inference. Next to the arguments the DatasetInference class will need to localize the data, it is also important to define the slice and the number of spokes which should be loaded from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetInference(\n",
    "    \"qrage/sessions/%s/preproc.zarr/preproc.zarr\",\n",
    "    [\"7T1566\"],\n",
    "    i_slice,\n",
    "    num_spokes,\n",
    "    endpoint_url=\"https://s3.fz-juelich.de\",\n",
    "    backend=\"s3\",\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the important data out of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize CheckpointManager and model and run the inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup CheckpointManager\n",
    "The CheckpointManager is used to load a trained model. First we do the setup and tell him where he has to search for the model files and then we can load specific properties out of the models directory.<br><br>\n",
    "#### Description of the complete CheckpointManager process:\n",
    "In the training notebook the CheckpointManager gets the input where it should save the model after every n itearations. When the model is trained and saved at the specific location, the CheckpointManger in the Inference notebook can load this model out of this specific location. In this way it is not only possible to use the completely trained model after nmax iterations but it is also possible to use the interim results to check if the model is going the right way.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_manager = CheckpointManager(\n",
    "    directory=directory,\n",
    "    root_dir=root_dir,\n",
    "    backend=backend,\n",
    ")\n",
    "\n",
    "checkpoint = checkpoint_manager.load(\n",
    "    [\"averaged_model_state\", \"iteration\"], map_location=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model and load a trained one with the CheckpointManager\n",
    "The model gets defined and calibrated for the specific image, that should be reconstructed. Afterwards a trained model will be loaded with the help of the CheckpointManager. Due to this, the parameters of the initialized model will be overwritten. At the end of the cell an output will be generated that gives information about the number of iterations the loaded model has been trained for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UnrolledNet(\n",
    "    shape,\n",
    "    features=features,\n",
    "    CG_Iter=cg_iter,\n",
    "    num_unroll_blocks=10,\n",
    "    spectral_normalization=False,\n",
    "    activation=\"ReLU\",\n",
    "    disable_progress_bar=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "model = ExponentialMovingAverageModel(model, 0.9)\n",
    "model.load_state_dict(checkpoint[\"averaged_model_state\"])\n",
    "\n",
    "iteration = checkpoint[\"iteration\"]\n",
    "print(f\"Loaded averaged at iteration {iteration}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image reconstruction with dl-qrage \n",
    "The MR image will be reconstructed with the data and the model we previously load. The device can be variated between the cpu and gpu. An Output will be generated which indicates the progress of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = inference(data, model, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to CG SENSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up and initialize CG SENSE reconstruction algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_block = DataConsistency(shape, device=device, axes=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    data[\"images_regridded\"].shape,\n",
    "    data[\"kspace_trajectory\"].shape,\n",
    "    data[\"sensitivity_maps\"].shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_block.init(\n",
    "    data[\"images_regridded\"],\n",
    "    data[\"kspace_trajectory\"],\n",
    "    sensitivity_maps=data[\"sensitivity_maps\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image reconstruction with CG SENSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    cg_sense = dc_block(data[\"images_regridded\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(np.abs(cg_sense[:, :, 0, iTI, iTE]), cmap=\"gray\", vmax=vmax)\n",
    "plt.title(\"CG-SENSE Recon\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(np.abs(images[:, :, 0, iTI, iTE]), cmap=\"gray\", vmax=vmax)\n",
    "plt.title(\"DL-QRAGE Recon\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resnet = ResNet(contrasts=4, dim=3)\n",
    "image = resnet(images)\n",
    "plt.imshow(np.abs((image[:, :, 0, iTI, iTE]).detach()), cmap=\"gray\", vmax=vmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d9f194ff65c618cbbfdc3e3ee346b4c37b387df0a6d9f1c7fc5fdac92c2846d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
