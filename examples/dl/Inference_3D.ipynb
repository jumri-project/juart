{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12c7a292-e8f9-4880-83cf-6a683b294b4b",
   "metadata": {},
   "source": [
    "# Inference Notebook for 3D models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b8ee3b-665b-41a6-85ee-ac083a6b3a78",
   "metadata": {},
   "source": [
    "## Importing all necessary classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e82023-148e-48a7-90ac-db663495bc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../src\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "\n",
    "from juart.dl.checkpoint.manager import CheckpointManager\n",
    "from juart.dl.data.inference import DatasetInference\n",
    "from juart.dl.model.dc import DataConsistency\n",
    "from juart.dl.model.unrollnet import ExponentialMovingAverageModel, UnrolledNet\n",
    "from juart.dl.operation.modules import inference\n",
    "from juart.dl.model.resnet import ResNet\n",
    "from juart.conopt.tfs.fourier import nonuniform_transfer_function\n",
    "from juart.conopt.functional.fourier import nonuniform_fourier_transform_adjoint, fourier_transform_forward, fourier_transform_adjoint\n",
    "from juart.vis.interactive import InteractiveFigure3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e32984-bb2c-46bd-89ab-318938bb852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To improve performance, manually limit the number of threads\n",
    "# torch.set_num_threads(16)\n",
    "# torch.set_num_interop_threads(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad64915-c5a6-404e-9a56-da165e68c8dc",
   "metadata": {},
   "source": [
    "## Defining all important variables for the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ee49da-f824-4e3c-8d6c-1332ecb41d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset options\n",
    "kspace_cutoff = False\n",
    "nX_cutoff, nY_cutoff, nZ_cutoff = 64, 64, 64 \n",
    "i_slice = [80] # number of slice that will be reconstructed\n",
    "num_spokes = 64 # number of spokes that are used for the reconstruction\n",
    "\n",
    "# device options\n",
    "device = \"cuda:2\" # device on which the reconstructions will run\n",
    "\n",
    "# CheckpointManager options\n",
    "directory = \"ray_test_32f\" # directory of the dl-qrage model\n",
    "root_dir = \"/home/jovyan/models\" # the path to the model directory\n",
    "backend = \"local\" # the backend of the model directory\n",
    "\n",
    "# model options\n",
    "nX, nY, nZ, nTI, nTE = (156, 156, 156, 2, 1) # number of pixels in every direction // number of measurements at T1/T2 decay\n",
    "shape = (nX, nY, nZ, nTI, nTE) # ordered structure of the parameters above that will be passed to the model\n",
    "features = 32 # number of hidden_inputs of the dl-qrage model\n",
    "cg_iter = 40 # number of cg iterations in the dl-qrage model reconstruction\n",
    "\n",
    "# display options\n",
    "vmax = 2 # sets the brightness normalization of the display between 0 and vmax\n",
    "iTI, iTE = 1, 0 # sets the number of measurement that should be displayed\n",
    "\n",
    "dtype = torch.complex32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322ab24-9d4f-49c0-b60d-bd75571f60c5",
   "metadata": {},
   "source": [
    "## Loading the dataset that should be reconstructed\n",
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931128c2-3025-409f-81d8-20f110fcad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"3DLiss_vd_preproc.h5\"\n",
    "with h5py.File(data_path, \"r\") as f:\n",
    "    print(f\"Dataset holds following data: {f.keys()}\")\n",
    "\n",
    "    print(f\"Coilsensitivity info: {f['coilsens'].attrs['info']}\")\n",
    "    print(f\"Trajectory info: {f['k'].attrs['info']}\")\n",
    "    print(f\"Signal info: {f['d'].attrs['info']}\")\n",
    "\n",
    "    shape = (156,156,156,2,1)\n",
    "    ktraj = torch.from_numpy(f['k'][:])\n",
    "    coilsens = torch.from_numpy(f['coilsens'][:])\n",
    "    d = torch.from_numpy(f['d'][:])\n",
    "\n",
    "    print(f\"Coilsensitivity shape {coilsens.shape}\")\n",
    "    print(f\"Trajectory shape {ktraj.shape}\")\n",
    "    print(f\"Signal shape {d.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a6150b-dcbf-4bfb-ae96-cbab3482474b",
   "metadata": {},
   "source": [
    "#### Shaping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd93fbe-605a-492a-babb-ef0494dcf27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if kspace_cutoff == True:\n",
    "    nX, nY ,nZ = nX_cutoff, nY_cutoff, nZ_cutoff\n",
    "    shape = (nX, nY, nZ, nTI, nTE)\n",
    "    \n",
    "    mask = torch.linalg.norm(ktraj,dim=0) <= nX_cutoff//2\n",
    "    \n",
    "    ktraj = torch.stack(\n",
    "        [ktraj[:, mask[:, echo], echo] for echo in range(ktraj.shape[2])],\n",
    "        dim=-1\n",
    "    )\n",
    "    \n",
    "    d = torch.stack(\n",
    "        [d[:, mask[:, echo], echo] for echo in range(d.shape[2])],\n",
    "        dim=-1\n",
    "    )\n",
    "\n",
    "    coilsens_ksp = fourier_transform_forward(coilsens, axes=(1,2,3))\n",
    "    low_lim, up_lim = int(156/2 - nX/2), int(156/2 + nX/2)\n",
    "    coilsens_ksp = coilsens_ksp[:, low_lim:up_lim, low_lim:up_lim, low_lim:up_lim]\n",
    "    coilsens = fourier_transform_adjoint(coilsens_ksp, axes=(1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8666b9-e8e4-4094-8283-70483e32bcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "kspace_mask_source = torch.randint(0,2,(1, ktraj.shape[1], 2,1))\n",
    "kspace_mask_target = 1 - kspace_mask_source\n",
    "\n",
    "k_scaled = ktraj / (2*ktraj.max())\n",
    "\n",
    "AHd = nonuniform_fourier_transform_adjoint(k_scaled,d,(nX,nY,nZ))\n",
    "AHd = torch.sum(torch.conj(coilsens[...,None]) * AHd, dim=0)\n",
    "\n",
    "AHd_unsqueeze = AHd.unsqueeze(-1)\n",
    "k = k_scaled.unsqueeze(-1)\n",
    "d_unsqueeze = d.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdc4c20-fa26-47a7-8737-272f96336fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"images_regridded\" : AHd_unsqueeze,\n",
    "        \"kspace_trajectory\" : k,\n",
    "        \"sensitivity_maps\" : coilsens,\n",
    "        \"kspace_mask_source\" : kspace_mask_source,\n",
    "        \"kspace_mask_target\" : kspace_mask_target,\n",
    "        \"kspace_data\" : d_unsqueeze}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020a4bfb-55c3-43d8-bf74-3aaece8cb014",
   "metadata": {},
   "source": [
    "## Checkpoint Manager\n",
    "#### Initializing the CheckpointManager and loading the current state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37572d0f-c884-435e-be7e-f674654fe50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_manager = CheckpointManager(\n",
    "    directory = directory,\n",
    "    root_dir = root_dir,\n",
    "    backend= backend,\n",
    ")\n",
    "\n",
    "checkpoint = checkpoint_manager.load(\n",
    "    [\"averaged_model_state\", \"iteration\"], map_location=\"cuda:3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1a869a-f5c0-4653-8240-c0604bd8d5fa",
   "metadata": {},
   "source": [
    "## Model\n",
    "#### Initializing the model and using the CheckpointManager to check for save files of the model and load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d67607-4fd4-4ec3-bea5-f6620bea7264",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UnrolledNet(\n",
    "    shape,\n",
    "    features=features,\n",
    "    CG_Iter=cg_iter,\n",
    "    num_unroll_blocks=10,\n",
    "    spectral_normalization=False,\n",
    "    activation=\"ReLU\",\n",
    "    kernel_size = (3,3,3),\n",
    "    axes = (1,2,3),\n",
    "    disable_progress_bar=False,\n",
    "    ResNetCheckpoints = True,\n",
    "    ConvLayerCheckpoints = False,\n",
    "    device=device,\n",
    "    dtype = dtype\n",
    ")\n",
    "\n",
    "model = ExponentialMovingAverageModel(model, 0.9)\n",
    "model.load_state_dict(checkpoint[\"averaged_model_state\"])\n",
    "\n",
    "iteration = checkpoint[\"iteration\"]\n",
    "print(f\"Loaded averaged at iteration {iteration}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af70eed7-0aa8-416b-86f3-97db681407f6",
   "metadata": {},
   "source": [
    "## Image reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3541e8e-62a4-4629-9fd2-9a27cdc57a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = inference(data, model, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991468ce-9d07-4c5a-9968-19f3f50c2c3a",
   "metadata": {},
   "source": [
    "### Displaying the reconstructed image in an interactive Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942463d6-48af-45a8-9c5a-a7f9e8e8b2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "InteractiveFigure3D(\n",
    "    torch.abs(images[...,0,0]).cpu().numpy(),\n",
    "    vmin=0,\n",
    "    vmax=torch.abs(images[...,0,0]).cpu().max(),\n",
    "    cmap=\"gray\",\n",
    ").interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3406fe04-0677-4cfd-a545-3b914af2945d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
