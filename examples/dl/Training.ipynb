{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "376ad082-4e9a-401e-9a7c-88f0a6f25b3f",
   "metadata": {},
   "source": [
    "# Training Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b52213f-77ba-4b82-aca7-78800499c2d4",
   "metadata": {},
   "source": [
    "## Import of all needed scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c1f36-daa3-450f-9d98-e1a96bd7281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../src\")\n",
    "\n",
    "import gc\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "from juart.dl.checkpoint.manager import CheckpointManager\n",
    "from juart.dl.data.training import DatasetTraining\n",
    "from juart.dl.loss.loss import JointLoss\n",
    "from juart.dl.model.unrollnet import (\n",
    "    LookaheadModel,\n",
    "    UnrolledNet,\n",
    ")\n",
    "from juart.dl.operation.modules import training, validation\n",
    "from juart.dl.utils.dist import GradientAccumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a280b4ee-1de8-481c-93a8-882b45e2fea4",
   "metadata": {},
   "source": [
    "## Defining shuffle function\n",
    "When training a model the slices and subjects of the datasets should not be in order. Therefore this function creates a random order for every single epoch, granting that every slice is used once and only once in every epoch. It is possible to give the function a seed so that the random order is the same order in every training. This is used for better comparability of the models, because the accuracy of the models can slightly variate with the order of the slices and subjects in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f4bce-3c55-40e9-b128-ae4986c86f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffled_indices(num_samples, num_epochs, rng):\n",
    "    indices = np.repeat(np.arange(num_samples), num_epochs)\n",
    "    indices = indices.reshape((num_samples, num_epochs))\n",
    "    indices = rng.permuted(indices, axis=0)\n",
    "    indices = indices.T.ravel()\n",
    "\n",
    "    # Check if each sample is used once and only once in every epoch\n",
    "    assert indices.size == num_samples * num_epochs\n",
    "    for i in np.split(indices, num_epochs):\n",
    "        assert np.unique(i).size == num_samples\n",
    "\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021c2169-2a1d-4683-8a63-a763f4058c7e",
   "metadata": {},
   "source": [
    "## Define important variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb11e6e-88ae-41db-9d3a-8a243df736d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset options\n",
    "nX, nY, nZ = 256, 256, 1  # Number of pixels in x-/y-/z-direction\n",
    "nTI, nTE = 2, 2  # Number of measurements during the T1/T2 decay\n",
    "shape = (nX, nY, nZ, nTI, nTE)  # Defining the shape later used for the model\n",
    "num_spokes = 64  # number of spokes that should be used for training\n",
    "nD = 1  # Number of subjects\n",
    "nS = 160  # Number of slices per subject\n",
    "\n",
    "# device options\n",
    "device = \"cpu\"  # defines whether the model should be trained on the cpu or gpu\n",
    "group = None\n",
    "group_rank = 0\n",
    "group_index = 0\n",
    "num_groups = 1\n",
    "\n",
    "# CheckpointManager Options\n",
    "load_model_state = True  # Load the last saved model state\n",
    "load_averaged_model_state = True  # Load the last saved averaged model state\n",
    "load_optim_state = True  # Load the las saved optimizer state\n",
    "load_metrics = True  # Load the last saved metrics (iterations and loss)\n",
    "directory = \"model_2D_test\"  # Name that is used for the save directory of the model\n",
    "root_dir = \"/home/jovyan/models\"  # path of the model directory\n",
    "backend = \"local\"  # backend of the model directory\n",
    "\n",
    "# Training loop options\n",
    "num_epochs = 5  # Number of epochs of the training\n",
    "model_training = True  # Activate Training mode\n",
    "model_validation = False  # Activate validation mode\n",
    "save_checkpoint = (\n",
    "    True  # Create save files that contain the current state of the training\n",
    ")\n",
    "checkpoint_frequency = 10  # Sets the number of iterations between creating save files\n",
    "single_epoch = True  # Create a seperate save file after every single epoch\n",
    "batch_size = 1  # Number of slices that should be used for training per batch\n",
    "\n",
    "batch_size_local = batch_size // num_groups\n",
    "num_iterations = nD * nS * num_epochs  # complete number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d5bdcb-b12a-4998-b790-454186ccc305",
   "metadata": {},
   "source": [
    "## Randomize indices\n",
    "The shuffle function gets used to shuffle the indices of the training and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d203d9-c38d-4171-afb6-bf9109a744a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=0)\n",
    "\n",
    "training_indices = shuffled_indices(nD * nS, num_epochs, rng)\n",
    "training_indices_batched = training_indices.reshape((-1, batch_size_local, num_groups))\n",
    "\n",
    "validation_indices = shuffled_indices(nD * nS, num_epochs, rng)\n",
    "validation_indices_batched = validation_indices.reshape(\n",
    "    (-1, batch_size_local, num_groups)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f6e48-096f-41a5-97fb-1a85e9743c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.init_process_group(\n",
    "    backend=\"gloo\", init_method=\"tcp://127.0.0.1:23456\", world_size=1, rank=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfdd93f-269a-4b81-82e2-ab084e908058",
   "metadata": {},
   "source": [
    "## Initializing the model\n",
    "In this cell the model gets initialized and set up for the specific dataset, that is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b313da1-7a4e-46ec-8215-1fa999ab102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UnrolledNet(\n",
    "    shape,\n",
    "    features=64,\n",
    "    CG_Iter=10,\n",
    "    num_unroll_blocks=10,\n",
    "    activation=\"ReLU\",\n",
    "    disable_progress_bar=True,\n",
    "    timing_level=0,\n",
    "    validation_level=0,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe7a61-9573-4013-b511-f472db2afb1a",
   "metadata": {},
   "source": [
    "## Defining the loss function\n",
    "The loss function takes the prediction of the model and the correct result to compute the resulting loss. The loss will be separated in different parts (kspace, ispace, wavelet,...) and then be weighted with the matching weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac1e029-5ce2-457a-853b-54545625aab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = JointLoss(\n",
    "    shape,\n",
    "    (3, 3),\n",
    "    weights_kspace_loss=(0.5, 0.5),\n",
    "    weights_ispace_loss=(0.0, 0.0),\n",
    "    weights_wavelet_loss=(0.0, 0.0),\n",
    "    weights_hankel_loss=(0.0, 0.0),\n",
    "    weights_casorati_loss=(0.0, 0.0),\n",
    "    normalized_loss=True,\n",
    "    timing_level=0,\n",
    "    validation_level=0,\n",
    "    group=group,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9f82bf-0b53-422d-8d26-96a8813f0781",
   "metadata": {},
   "source": [
    "## Setting up the optimizer\n",
    "The optimizer is used for adapting the model parameters so that the prediction of the model will get closer to the correct result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406e2d52-dcbe-4783-bf8f-170c0c1fd728",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.0001,\n",
    "    betas=[0.9, 0.999],\n",
    "    eps=1.0e-8,\n",
    "    weight_decay=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b74295-cab2-480d-9433-f2fea449277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulator = GradientAccumulator(\n",
    "    model,\n",
    "    accumulation_steps=batch_size_local,\n",
    "    max_norm=1.0,\n",
    "    normalized_gradient=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa72d410-53aa-4cc5-ad4d-7f11779e3652",
   "metadata": {},
   "source": [
    "## The average model\n",
    "The average model uses the floating average of the original model. This approach is more robust then using the original model directly for the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb368c3e-b524-4fb3-8ed6-389e876a7662",
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_model = LookaheadModel(\n",
    "    model,\n",
    "    alpha=0.5,\n",
    "    k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e1a80-a277-4975-996a-679bc499cf99",
   "metadata": {},
   "source": [
    "## Initializing the CheckpointManager\n",
    "The CheckpointManager allows to save the model in a specific location or load a model from a specific location. Here the location is defined in which the CheckpointManager is operating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64a1b7-d2d2-4578-8f26-ebf1dc5b6b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_manager = CheckpointManager(\n",
    "    directory=directory,\n",
    "    root_dir=root_dir,\n",
    "    backend=backend,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714e51d2-898e-4255-9f55-d7fffe26e07b",
   "metadata": {},
   "source": [
    "## Loading save files with the CheckpointManager if available\n",
    "### Load saved model\n",
    "The following cell will test if theres a saved model_state already. It will load the current state and will provide that the training process will continue at the same point where it was saved the last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65999c4b-92d0-4433-a8bc-074e707f390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model_state:\n",
    "    print(\"Loading model state ...\")\n",
    "    checkpoint = checkpoint_manager.load([\"model_state\"], map_location=device)\n",
    "    if all(checkpoint.values()):\n",
    "        model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    else:\n",
    "        print(\"Could not load model state.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3108467-4c90-4fdc-ab57-ec25ce7802e1",
   "metadata": {},
   "source": [
    "### Load saved averaged model\n",
    "The next cell will do the same thing but just for the averaged model and not for the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821761fa-be45-4635-957a-4165a6ef573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_averaged_model_state:\n",
    "    print(\"Loading averaged model state ...\")\n",
    "    checkpoint = checkpoint_manager.load([\"averaged_model_state\"], map_location=device)\n",
    "    if all(checkpoint.values()):\n",
    "        averaged_model.load_state_dict(checkpoint[\"averaged_model_state\"])\n",
    "    else:\n",
    "        print(\"Could not load averaged model state.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bc42d9-b62c-4e77-9384-2f3d94e7a8b2",
   "metadata": {},
   "source": [
    "### Load saved optimizer\n",
    "This cell will load the last saved state of the optimizer of the saved model, so that the optimizer parameters, which where achieved through previous iterations, still have their impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c48de6-e87a-4a90-ba43-0aef7df8664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_optim_state:\n",
    "    print(\"Loading optim state ...\")\n",
    "    checkpoint = checkpoint_manager.load([\"optim_state\"], map_location=device)\n",
    "    if all(checkpoint.values()):\n",
    "        optimizer.load_state_dict(checkpoint[\"optim_state\"])\n",
    "    else:\n",
    "        print(\"Could not load optim state.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3333df6d-18e3-4a15-85e9-3d0deaa5a8af",
   "metadata": {},
   "source": [
    "### Load saved metrics\n",
    "In the following cells the last saved loss and the last saved iteration of the saved model will be loaded from the save files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424ede94-dc37-4db4-9c3a-a24773169f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trn_loss = list()\n",
    "total_val_loss = list()\n",
    "iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7724ebad-0b3a-4d80-8d33-754b42db55ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_metrics:\n",
    "    print(\"Loading metrics ...\")\n",
    "    checkpoint = checkpoint_manager.load([\"trn_loss\", \"val_loss\", \"iteration\"])\n",
    "    if all(checkpoint.values()):\n",
    "        total_trn_loss = list(checkpoint[\"trn_loss\"])\n",
    "        total_val_loss = list(checkpoint[\"val_loss\"])\n",
    "        iteration = checkpoint[\"iteration\"]\n",
    "    else:\n",
    "        print(\"Could not load metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11d6e67-7b85-407d-8e53-358499d0c5a8",
   "metadata": {},
   "source": [
    "This cells gives an output which will have the information about the last saved iteration from the model. The training will continue at this iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2937b104-b42d-4c58-bb2b-722705a0e39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Continue with iteration {iteration} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0f7966-6707-4ced-9211-2da011c6a095",
   "metadata": {},
   "source": [
    "## Loading Dataset for Training and Validation\n",
    "The dataset can be varied in the number of slices, number of spokes, split_fractions and mode (training/validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cb5fd6-2278-4477-b8ed-58450a041abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = DatasetTraining(\n",
    "    \"qrage/sessions/%s/preproc.zarr/preproc.zarr\",\n",
    "    [\"7T1566\"],\n",
    "    np.arange(0, 160),\n",
    "    num_spokes,\n",
    "    [0.0, 0.5, 0.5],\n",
    "    mode=\"training\",\n",
    "    group_rank=group_rank,\n",
    "    endpoint_url=\"https://s3.fz-juelich.de\",\n",
    "    backend=\"s3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390ac877-4988-492e-9da6-91836d624fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = DatasetTraining(\n",
    "    \"qrage/sessions/%s/preproc.zarr/preproc.zarr\",\n",
    "    [\"7T1566\"],\n",
    "    np.arange(0, 160),\n",
    "    num_spokes,\n",
    "    [0.0, 0.5, 0.5],\n",
    "    mode=\"validation\",\n",
    "    group_rank=group_rank,\n",
    "    endpoint_url=\"https://s3.fz-juelich.de\",\n",
    "    backend=\"s3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a0cfd4-9abd-46f3-82e0-2f27b1a011fb",
   "metadata": {},
   "source": [
    "## Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32731745-4018-40e5-851d-41e44cdc10ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "while iteration < num_iterations:\n",
    "    tic = time.time()\n",
    "\n",
    "    # Reset the seed so that training can be resumed\n",
    "    np.random.seed(iteration)\n",
    "    torch.manual_seed(iteration)\n",
    "\n",
    "    training_index = training_indices_batched[\n",
    "        iteration // batch_size,\n",
    "        :,\n",
    "        group_index,\n",
    "    ].tolist()\n",
    "    validation_index = validation_indices_batched[\n",
    "        iteration // batch_size, :, group_index\n",
    "    ].tolist()\n",
    "\n",
    "    # TRAINING\n",
    "    if model_training:\n",
    "        print(f\"Training index {training_index} ...\")\n",
    "\n",
    "        trn_loss = training(\n",
    "            training_index,\n",
    "            training_data,\n",
    "            model,\n",
    "            loss_fn,\n",
    "            optimizer,\n",
    "            accumulator,\n",
    "            group=group,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        averaged_model.update_parameters(\n",
    "            model,\n",
    "        )\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    else:\n",
    "        trn_loss = [0] * batch_size\n",
    "\n",
    "    # VALIDATION\n",
    "    if model_validation:\n",
    "        print(f\"Validation index {validation_index} ...\")\n",
    "\n",
    "        val_loss = validation(\n",
    "            validation_index,\n",
    "            validation_data,\n",
    "            averaged_model,\n",
    "            loss_fn,\n",
    "            group=group,\n",
    "            device=device,\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    else:\n",
    "        val_loss = [0] * batch_size\n",
    "\n",
    "    total_trn_loss += trn_loss\n",
    "    total_val_loss += val_loss\n",
    "\n",
    "    # SAVING\n",
    "    # Completed epoch\n",
    "    if save_checkpoint and np.mod(iteration + batch_size, nD * nS) == 0:\n",
    "        print(\"Creating tagged checkpoint ...\")\n",
    "\n",
    "        checkpoint = {\n",
    "            \"iteration\": iteration + batch_size,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"averaged_model_state\": averaged_model.state_dict(),\n",
    "            \"optim_state\": optimizer.state_dict(),\n",
    "            \"trn_loss\": total_trn_loss,\n",
    "            \"val_loss\": total_val_loss,\n",
    "        }\n",
    "\n",
    "        epoch = (iteration + batch_size) // (nD * nS)\n",
    "        checkpoint_manager.save(checkpoint, tag=f\"_epoch_{epoch}\")\n",
    "\n",
    "        if single_epoch:\n",
    "            # Also save the checkpoint as untagged checkpoint\n",
    "            # Otherwise, training will be stuck in endless loop\n",
    "            checkpoint_manager.save(checkpoint)\n",
    "            checkpoint_manager.release()\n",
    "            break\n",
    "\n",
    "    # Intermediate checkpoint\n",
    "    elif save_checkpoint and np.mod(iteration + batch_size, checkpoint_frequency) == 0:\n",
    "        print(\"Creating untagged checkpoint ...\")\n",
    "\n",
    "        checkpoint = {\n",
    "            \"iteration\": iteration + batch_size,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"averaged_model_state\": averaged_model.state_dict(),\n",
    "            \"optim_state\": optimizer.state_dict(),\n",
    "            \"trn_loss\": total_trn_loss,\n",
    "            \"val_loss\": total_val_loss,\n",
    "        }\n",
    "\n",
    "        checkpoint_manager.save(checkpoint, block=False)\n",
    "\n",
    "    toc = time.time() - tic\n",
    "\n",
    "    print(\n",
    "        (\n",
    "            f\"Iteration: {iteration} - \"\n",
    "            + f\"Elapsed time: {toc:.0f} - \"\n",
    "            + f\"Training loss: {[f'{loss:.3f}' for loss in trn_loss]} - \"\n",
    "            + f\"Validation loss: {[f'{loss:.3f}' for loss in val_loss]}\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    iteration += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f33a07d-a000-4045-89db-dad07ba5e894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
