{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "376ad082-4e9a-401e-9a7c-88f0a6f25b3f",
   "metadata": {},
   "source": [
    "# Training Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b52213f-77ba-4b82-aca7-78800499c2d4",
   "metadata": {},
   "source": [
    "## Import of all needed scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f49c1f36-daa3-450f-9d98-e1a96bd7281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../src\")\n",
    "\n",
    "import gc\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "from juart.dl.checkpoint.manager import CheckpointManager\n",
    "from juart.dl.data.training import DatasetTraining\n",
    "from juart.dl.loss.loss import JointLoss\n",
    "from juart.dl.model.unrollnet import (\n",
    "    LookaheadModel,\n",
    "    UnrolledNet,\n",
    ")\n",
    "from juart.dl.operation.modules import training, validation\n",
    "from juart.dl.utils.dist import GradientAccumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a280b4ee-1de8-481c-93a8-882b45e2fea4",
   "metadata": {},
   "source": [
    "## Defining shuffle function\n",
    "When training a model the slices and subjects of the datasets should not be in order. Therefore this function creates a random order for every single epoch, granting that every sclice is used once and only once in every epoch. It is possible to give the function a seed so that the random order is the same order in every training. This is used for better comparability of the models, because the accuracy of the models can slightly variate with the order of the slices and subjects in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "617f4bce-3c55-40e9-b128-ae4986c86f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffled_indices(num_samples, num_epochs, rng):\n",
    "    indices = np.repeat(np.arange(num_samples), num_epochs)\n",
    "    indices = indices.reshape((num_samples, num_epochs))\n",
    "    indices = rng.permuted(indices, axis=0)\n",
    "    indices = indices.T.ravel()\n",
    "\n",
    "    # Check if each sample is used once and only once in every epoch\n",
    "    assert indices.size == num_samples * num_epochs\n",
    "    for i in np.split(indices, num_epochs):\n",
    "        assert np.unique(i).size == num_samples\n",
    "\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021c2169-2a1d-4683-8a63-a763f4058c7e",
   "metadata": {},
   "source": [
    "## Define important variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb11e6e-88ae-41db-9d3a-8a243df736d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nX, nY = 256, 256  # Number of pixels in x-/y-direction\n",
    "nTI, nTE = 2, 2  # Number of measurements during the T1/T2 decay\n",
    "device = \"cpu\"  # defines whether the model should be trained on the cpu or gpu\n",
    "group = None  # ?\n",
    "group_rank = 0  # ?\n",
    "group_index = 0  # ?\n",
    "\n",
    "num_epochs = 1  # Number of epochs of the training\n",
    "num_groups = 1  # ?\n",
    "batch_size = 1  # Number of slices that should be used for training per batch\n",
    "nD = 1  # Number of subjects\n",
    "nS = 160  # Number of slices per subject\n",
    "\n",
    "batch_size_local = batch_size // num_groups\n",
    "\n",
    "num_iterations = nD * nS * num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d5bdcb-b12a-4998-b790-454186ccc305",
   "metadata": {},
   "source": [
    "## Randomize indices\n",
    "The shuffle function gets used to shuffle the indices of the training and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17d203d9-c38d-4171-afb6-bf9109a744a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=0)\n",
    "\n",
    "training_indices = shuffled_indices(nD * nS, num_epochs, rng)\n",
    "training_indices_batched = training_indices.reshape((-1, batch_size_local, num_groups))\n",
    "\n",
    "validation_indices = shuffled_indices(nD * nS, num_epochs, rng)\n",
    "validation_indices_batched = validation_indices.reshape(\n",
    "    (-1, batch_size_local, num_groups)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f29f6e48-096f-41a5-97fb-1a85e9743c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    }
   ],
   "source": [
    "dist.init_process_group(\n",
    "    backend=\"gloo\", init_method=\"tcp://127.0.0.1:23456\", world_size=1, rank=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b313da1-7a4e-46ec-8215-1fa999ab102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UnrolledNet(\n",
    "    (nX, nY),\n",
    "    contrasts=nTI * nTE,\n",
    "    features=64,\n",
    "    CG_Iter=10,\n",
    "    num_unroll_blocks=10,\n",
    "    activation=\"ReLU\",\n",
    "    disable_progress_bar=True,\n",
    "    timing_level=0,\n",
    "    validation_level=0,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ac1e029-5ce2-457a-853b-54545625aab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = JointLoss(\n",
    "    (nX, nY),\n",
    "    (3, 3),\n",
    "    weights_kspace_loss=(0.5, 0.5),\n",
    "    weights_ispace_loss=(0.0, 0.0),\n",
    "    weights_wavelet_loss=(0.0, 0.0),\n",
    "    weights_hankel_loss=(0.0, 0.0),\n",
    "    weights_casorati_loss=(0.0, 0.0),\n",
    "    normalized_loss=True,\n",
    "    timing_level=0,\n",
    "    validation_level=0,\n",
    "    group=group,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "406e2d52-dcbe-4783-bf8f-170c0c1fd728",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.0001,\n",
    "    betas=[0.9, 0.999],\n",
    "    eps=1.0e-8,\n",
    "    weight_decay=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60b74295-cab2-480d-9433-f2fea449277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulator = GradientAccumulator(\n",
    "    model,\n",
    "    accumulation_steps=batch_size_local,\n",
    "    max_norm=1.0,\n",
    "    normalized_gradient=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb368c3e-b524-4fb3-8ed6-389e876a7662",
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_model = LookaheadModel(\n",
    "    model,\n",
    "    alpha=0.5,\n",
    "    k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b64a1b7-d2d2-4578-8f26-ebf1dc5b6b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_manager = CheckpointManager(\n",
    "    directory=\"model_8spokes\",\n",
    "    root_dir=\"/home/jovyan/models\",\n",
    "    backend=\"local\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5effe04-5bff-4973-8b07-2afdf8bcbe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model_state = True\n",
    "load_averaged_model_state = True\n",
    "load_optim_state = True\n",
    "load_metrics = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65999c4b-92d0-4433-a8bc-074e707f390f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model state ...\n",
      "Could not load model state.\n"
     ]
    }
   ],
   "source": [
    "if load_model_state:\n",
    "    print(\"Loading model state ...\")\n",
    "    checkpoint = checkpoint_manager.load([\"model_state\"], map_location=device)\n",
    "    if all(checkpoint.values()):\n",
    "        model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    else:\n",
    "        print(\"Could not load model state.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "821761fa-be45-4635-957a-4165a6ef573e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading averaged model state ...\n",
      "Could not load averaged model state.\n"
     ]
    }
   ],
   "source": [
    "if load_averaged_model_state:\n",
    "    print(\"Loading averaged model state ...\")\n",
    "    checkpoint = checkpoint_manager.load([\"averaged_model_state\"], map_location=device)\n",
    "    if all(checkpoint.values()):\n",
    "        averaged_model.load_state_dict(checkpoint[\"averaged_model_state\"])\n",
    "    else:\n",
    "        print(\"Could not load averaged model state.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23c48de6-e87a-4a90-ba43-0aef7df8664e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading optim state ...\n",
      "Could not load optim state.\n"
     ]
    }
   ],
   "source": [
    "if load_optim_state:\n",
    "    print(\"Loading optim state ...\")\n",
    "    checkpoint = checkpoint_manager.load([\"optim_state\"], map_location=device)\n",
    "    if all(checkpoint.values()):\n",
    "        optimizer.load_state_dict(checkpoint[\"optim_state\"])\n",
    "    else:\n",
    "        print(\"Could not load optim state.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "424ede94-dc37-4db4-9c3a-a24773169f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trn_loss = list()\n",
    "total_val_loss = list()\n",
    "iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7724ebad-0b3a-4d80-8d33-754b42db55ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metrics ...\n",
      "Could not load metrics.\n"
     ]
    }
   ],
   "source": [
    "if load_metrics:\n",
    "    print(\"Loading metrics ...\")\n",
    "    checkpoint = checkpoint_manager.load([\"trn_loss\", \"val_loss\", \"iteration\"])\n",
    "    if all(checkpoint.values()):\n",
    "        total_trn_loss = list(checkpoint[\"trn_loss\"])\n",
    "        total_val_loss = list(checkpoint[\"val_loss\"])\n",
    "        iteration = checkpoint[\"iteration\"]\n",
    "    else:\n",
    "        print(\"Could not load metrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2937b104-b42d-4c58-bb2b-722705a0e39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue with iteration 0 ...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Continue with iteration {iteration} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10cb5fd6-2278-4477-b8ed-58450a041abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = DatasetTraining(\n",
    "    \"qrage/sessions/%s/preproc.zarr/preproc.zarr\",\n",
    "    [\"7T1566\"],\n",
    "    np.arange(0, 160),\n",
    "    8,\n",
    "    [0.0, 0.5, 0.5],\n",
    "    mode=\"training\",\n",
    "    group_rank=group_rank,\n",
    "    endpoint_url=\"https://s3.fz-juelich.de\",\n",
    "    backend=\"s3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "390ac877-4988-492e-9da6-91836d624fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = DatasetTraining(\n",
    "    \"qrage/sessions/%s/preproc.zarr/preproc.zarr\",\n",
    "    [\"7T1566\"],\n",
    "    np.arange(0, 160),\n",
    "    8,\n",
    "    [0.0, 0.5, 0.5],\n",
    "    mode=\"validation\",\n",
    "    group_rank=group_rank,\n",
    "    endpoint_url=\"https://s3.fz-juelich.de\",\n",
    "    backend=\"s3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32731745-4018-40e5-851d-41e44cdc10ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training index [10] ...\n",
      "Data - Started loading Dataset 7T1566 - Slice 10 ...\n",
      "Data - Completed loading dataset in 3.6 seconds.\n",
      "Rank 0 - Data - Started creating masks [0.0, 0.5, 0.5] ...\n",
      "Rank 0 - Data - Source fractions 0.5.\n",
      "Rank 0 - Data - Target fractions 0.5.\n",
      "Rank 0 - Data - Completed creating mask torch.Size([3, 1, 2048, 2, 2]) in 0.0 seconds.\n",
      "Rank 0 - Data - Started regridding dataset ...\n",
      "Rank 0 - Data - Completed regridding dataset torch.Size([256, 256, 1, 2, 2]) in 0.2 seconds.\n",
      "[KSpaceLoss] torch.Size([8, 2048, 2, 2]) torch.Size([8, 2048, 2, 2]) None\n",
      "Rank 0 - Loss k-space: 0.700 - Loss image space: 0.000 - Loss Wavelet 0.000 - Loss Hankel 0.000 - Loss Casorati 0.000\n",
      "Rank 0 - Index 0 - Gradient norm: 3.264\n",
      "Averaged gradient norm: 1.000\n",
      "Iteration: 0 - Elapsed time: 82 - Training loss: ['0.700'] - Validation loss: ['0.000']\n",
      "Training index [139] ...\n",
      "Data - Started loading Dataset 7T1566 - Slice 139 ...\n",
      "Data - Completed loading dataset in 2.9 seconds.\n",
      "Rank 0 - Data - Started creating masks [0.0, 0.5, 0.5] ...\n",
      "Rank 0 - Data - Source fractions 0.5.\n",
      "Rank 0 - Data - Target fractions 0.5.\n",
      "Rank 0 - Data - Completed creating mask torch.Size([3, 1, 2048, 2, 2]) in 0.0 seconds.\n",
      "Rank 0 - Data - Started regridding dataset ...\n",
      "Rank 0 - Data - Completed regridding dataset torch.Size([256, 256, 1, 2, 2]) in 0.2 seconds.\n",
      "[KSpaceLoss] torch.Size([8, 2048, 2, 2]) torch.Size([8, 2048, 2, 2]) None\n",
      "Rank 0 - Loss k-space: 0.604 - Loss image space: 0.000 - Loss Wavelet 0.000 - Loss Hankel 0.000 - Loss Casorati 0.000\n"
     ]
    }
   ],
   "source": [
    "while iteration < num_iterations:\n",
    "    tic = time.time()\n",
    "\n",
    "    # Reset the seed so that training can be resumed\n",
    "    np.random.seed(iteration)\n",
    "    torch.manual_seed(iteration)\n",
    "\n",
    "    training_index = training_indices_batched[\n",
    "        iteration // batch_size,\n",
    "        :,\n",
    "        group_index,\n",
    "    ].tolist()\n",
    "    validation_index = validation_indices_batched[\n",
    "        iteration // batch_size, :, group_index\n",
    "    ].tolist()\n",
    "\n",
    "    if True:  # options[\"model_training\"]:\n",
    "        print(f\"Training index {training_index} ...\")\n",
    "\n",
    "        trn_loss = training(\n",
    "            training_index,\n",
    "            training_data,\n",
    "            model,\n",
    "            loss_fn,\n",
    "            optimizer,\n",
    "            accumulator,\n",
    "            group=group,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        averaged_model.update_parameters(\n",
    "            model,\n",
    "        )\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    else:\n",
    "        trn_loss = [0] * batch_size\n",
    "\n",
    "    if False:  # options[\"model_validation\"]:\n",
    "        print(f\"Validation index {validation_index} ...\")\n",
    "\n",
    "        val_loss = validation(\n",
    "            validation_index,\n",
    "            validation_data,\n",
    "            averaged_model,\n",
    "            loss_fn,\n",
    "            group=group,\n",
    "            device=device,\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    else:\n",
    "        val_loss = [0] * batch_size\n",
    "\n",
    "    total_trn_loss += trn_loss\n",
    "    total_val_loss += val_loss\n",
    "\n",
    "    # Completed epoch\n",
    "    if (\n",
    "        True  # options[\"save_checkpoint\"]\n",
    "        and np.mod(iteration + batch_size, nD * nS) == 0\n",
    "    ):\n",
    "        print(\"Creating tagged checkpoint ...\")\n",
    "\n",
    "        checkpoint = {\n",
    "            \"iteration\": iteration + batch_size,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"averaged_model_state\": averaged_model.state_dict(),\n",
    "            \"optim_state\": optimizer.state_dict(),\n",
    "            \"trn_loss\": total_trn_loss,\n",
    "            \"val_loss\": total_val_loss,\n",
    "        }\n",
    "\n",
    "        epoch = (iteration + batch_size) // (nD * nS)\n",
    "        checkpoint_manager.save(checkpoint, tag=f\"_epoch_{epoch}\")\n",
    "\n",
    "        if True:  # options[\"single_epoch\"]\n",
    "            # Also save the checkpoint as untagged checkpoint\n",
    "            # Otherwise, training will be stuck in endless loop\n",
    "            checkpoint_manager.save(checkpoint)\n",
    "            checkpoint_manager.release()\n",
    "            break\n",
    "\n",
    "    # Intermediate checkpoint\n",
    "    elif (\n",
    "        True  # options[\"save_checkpoint\"]\n",
    "        and np.mod(iteration + batch_size, 10)\n",
    "        == 0  # 10 = options[\"checkpoint_frequency\"]\n",
    "    ):\n",
    "        print(\"Creating untagged checkpoint ...\")\n",
    "\n",
    "        checkpoint = {\n",
    "            \"iteration\": iteration + batch_size,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"averaged_model_state\": averaged_model.state_dict(),\n",
    "            \"optim_state\": optimizer.state_dict(),\n",
    "            \"trn_loss\": total_trn_loss,\n",
    "            \"val_loss\": total_val_loss,\n",
    "        }\n",
    "\n",
    "        checkpoint_manager.save(checkpoint, block=False)\n",
    "\n",
    "    toc = time.time() - tic\n",
    "\n",
    "    print(\n",
    "        (\n",
    "            f\"Iteration: {iteration} - \"\n",
    "            + f\"Elapsed time: {toc:.0f} - \"\n",
    "            + f\"Training loss: {[f'{loss:.3f}' for loss in trn_loss]} - \"\n",
    "            + f\"Validation loss: {[f'{loss:.3f}' for loss in val_loss]}\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    iteration += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f33a07d-a000-4045-89db-dad07ba5e894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
