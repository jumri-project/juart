{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c61772-828b-44ab-a299-33ec910180c2",
   "metadata": {},
   "source": [
    "# Dual Domain Training for 3D Datasets\n",
    "This Notebook is an upgraded version of the already existing Training_3D.ipynb. The reconstructed images of the models trained by the normal training seem to be very noisy. The dual domain training can hopefully remove the noise better. The structure of the Network stays the same but ray and the torch distributor function are used to run the training twice on two different gpus. Furthermore the ispace loss is used to weight the difference between the kspace data of the 2 models and to average their gradients before doing the optimizer step. In this way both trainings are running seperately but the optimizer step they do are the exact same because the greadients used during the optimizers step are the average of the both calculated gradients. Setting the weight_ispace_loss on 0 results in the normal Training again (except of the fact that there are 2 models getting trained but only one is saved at the end)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61a03f1b-c822-497c-9317-bbef848cb246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-24 09:19:58,355\tWARNING services.py:2148 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 65998848 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2025-09-24 09:19:58,590\tINFO worker.py:1942 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-09-24 09:19:58,689\tINFO packaging.py:588 -- Creating a file package for local module '/home/jovyan/juart/src'.\n",
      "2025-09-24 09:19:58,725\tWARNING packaging.py:430 -- File /home/jovyan/juart/src/juart/phantoms/mni/MNIbrain_hires.nii is very large (54.24MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/jovyan/juart/src/juart/phantoms/mni/MNIbrain_hires.nii']})`\n",
      "2025-09-24 09:19:58,985\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_ebf3d0d5f95cf163.zip' (70.09MiB) to Ray cluster...\n",
      "2025-09-24 09:19:59,479\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_ebf3d0d5f95cf163.zip'.\n",
      "/opt/conda/lib/python3.13/site-packages/ray/train/base_trainer.py:575: RayDeprecationWarning: `ray.train.RunConfig(verbose)` is deprecated. This parameter controls Ray Tune logging verbosity, and is only relevant when using Ray Tune. This parameter is still available in `ray.tune.RunConfig` for passing into a `ray.tune.Tuner`. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "  _log_deprecation_warning(VERBOSE_DEPRECATION_MESSAGE)\n",
      "2025-09-24 09:20:04,544\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2025-09-24 09:20:04,548\tINFO tensorboardx.py:193 -- pip install \"ray[tune]\" to see TensorBoard files.\n",
      "2025-09-24 09:20:04,548\tWARNING callback.py:143 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-09-24 09:20:04 (running for 00:00:00.12)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/128 CPUs, 0/2 GPUs (0.0/1.0 accelerator_type:A100)\n",
      "Result logdir: /tmp/ray/session_2025-09-24_09-19-53_758497_4165116/artifacts/2025-09-24_09-20-04/torch_trainer_example/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-09-24 09:20:09 (running for 00:00:05.18)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 49.0/128 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:A100)\n",
      "Result logdir: /tmp/ray/session_2025-09-24_09-19-53_758497_4165116/artifacts/2025-09-24_09-20-04/torch_trainer_example/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TorchTrainer pid=4175346)\u001b[0m /opt/conda/lib/python3.13/site-packages/ray/train/base_trainer.py:575: RayDeprecationWarning: `ray.train.RunConfig(verbose)` is deprecated. This parameter controls Ray Tune logging verbosity, and is only relevant when using Ray Tune. This parameter is still available in `ray.tune.RunConfig` for passing into a `ray.tune.Tuner`. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(TorchTrainer pid=4175346)\u001b[0m   _log_deprecation_warning(VERBOSE_DEPRECATION_MESSAGE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-09-24 09:20:14 (running for 00:00:10.21)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 49.0/128 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:A100)\n",
      "Result logdir: /tmp/ray/session_2025-09-24_09-19-53_758497_4165116/artifacts/2025-09-24_09-20-04/torch_trainer_example/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4178069)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[36m(TorchTrainer pid=4175346)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=4175346)\u001b[0m - (node_id=f21ca5a7638fd180f35278a2c123d9ee76196dd84493b25f8de7ecf7, ip=10.1.65.9, pid=4178069) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=4175346)\u001b[0m - (node_id=f21ca5a7638fd180f35278a2c123d9ee76196dd84493b25f8de7ecf7, ip=10.1.65.9, pid=4178068) world_rank=1, local_rank=1, node_rank=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-09-24 09:20:19 (running for 00:00:15.23)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 49.0/128 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:A100)\n",
      "Result logdir: /tmp/ray/session_2025-09-24_09-19-53_758497_4165116/artifacts/2025-09-24_09-20-04/torch_trainer_example/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=4178069)\u001b[0m Rank 0 is using devide cuda:0 ...\n",
      "\u001b[36m(RayTrainWorker pid=4178069)\u001b[0m Rank 0 is in group [0, 1] ...\n",
      "== Status ==\n",
      "Current time: 2025-09-24 09:20:24 (running for 00:00:20.25)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 49.0/128 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:A100)\n",
      "Result logdir: /tmp/ray/session_2025-09-24_09-19-53_758497_4165116/artifacts/2025-09-24_09-20-04/torch_trainer_example/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4178068)\u001b[0m /opt/conda/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "\u001b[36m(RayTrainWorker pid=4178068)\u001b[0m   warnings.warn(  # warn only once\n",
      "\u001b[36m(RayTrainWorker pid=4178068)\u001b[0m [rank1]:[W924 09:20:25.884361458 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4178069)\u001b[0m Rank 0 - Loading model state ...\n",
      "\u001b[36m(RayTrainWorker pid=4178069)\u001b[0m Rank 0 - Loading averaged model state ...\n",
      "\u001b[36m(RayTrainWorker pid=4178069)\u001b[0m Rank 0 - Loading optim state ...\n",
      "\u001b[36m(RayTrainWorker pid=4178068)\u001b[0m Rank 1 is using devide cuda:1 ...\n",
      "\u001b[36m(RayTrainWorker pid=4178068)\u001b[0m Rank 1 is in group [0, 1] ...\n",
      "== Status ==\n",
      "Current time: 2025-09-24 09:20:29 (running for 00:00:25.27)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 49.0/128 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:A100)\n",
      "Result logdir: /tmp/ray/session_2025-09-24_09-19-53_758497_4165116/artifacts/2025-09-24_09-20-04/torch_trainer_example/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=4178069)\u001b[0m Rank 0 - Loading metrics ...\n",
      "\u001b[36m(RayTrainWorker pid=4178069)\u001b[0m Rank 0 - Could not load metrics.\n",
      "\u001b[36m(RayTrainWorker pid=4178069)\u001b[0m Rank 0 - Continue with iteration 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-24 09:20:30,279\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_a7bd8_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.13/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/opt/conda/lib/python3.13/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.13/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.13/site-packages/ray/_private/worker.py\", line 2882, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.13/site-packages/ray/_private/worker.py\", line 968, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=4175346, ip=10.1.65.9, actor_id=6f0bb06763ac8ade69f371bf01000000, repr=TorchTrainer)\n",
      "  File \"/opt/conda/lib/python3.13/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/opt/conda/lib/python3.13/site-packages/ray/train/_internal/utils.py\", line 57, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "    ~~~~~~~^^^^^^^^^^^^\n",
      "                                  ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=4178069, ip=10.1.65.9, actor_id=154138ab755d601f05ecd68a01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7fdad6762f90>)\n",
      "  File \"/opt/conda/lib/python3.13/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/opt/conda/lib/python3.13/site-packages/ray/train/_internal/utils.py\", line 176, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "    ~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_4165116/378395350.py\", line 221, in train_func\n",
      "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n",
      "2025-09-24 09:20:30,288\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/jovyan/ray_results/torch_trainer_example' in 0.0026s.\n",
      "2025-09-24 09:20:30,292\tERROR tune.py:1037 -- Trials did not complete: [TorchTrainer_a7bd8_00000]\n",
      "2025-09-24 09:20:30,293\tINFO tune.py:1041 -- Total run time: 25.75 seconds (25.73 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-09-24 09:20:30 (running for 00:00:25.73)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 49.0/128 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:A100)\n",
      "Result logdir: /tmp/ray/session_2025-09-24_09-19-53_758497_4165116/artifacts/2025-09-24_09-20-04/torch_trainer_example/driver_artifacts\n",
      "Number of trials: 1/1 (1 ERROR)\n",
      "Number of errored trials: 1\n",
      "+--------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name               |   # failures | error file                                                                                                                                                                        |\n",
      "|--------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| TorchTrainer_a7bd8_00000 |            1 | /tmp/ray/session_2025-09-24_09-19-53_758497_4165116/artifacts/2025-09-24_09-20-04/torch_trainer_example/driver_artifacts/TorchTrainer_a7bd8_00000_0_2025-09-24_09-20-04/error.txt |\n",
      "+--------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "TrainingFailedError",
     "evalue": "The Ray Train run failed. Please inspect the previous error messages for a cause. After fixing the issue (assuming that the error is not caused by your own application logic, but rather an error such as OOM), you can restart the run from scratch or continue this run.\nTo continue this run, you can use: `trainer = TorchTrainer.restore(\"/home/jovyan/ray_results/torch_trainer_example\")`.\nTo start a new run that will retry on training failures, set `train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for unlimited retries.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRayTaskError(RuntimeError)\u001b[39m                Traceback (most recent call last)",
      "\u001b[31mRayTaskError(RuntimeError)\u001b[39m: \u001b[36mray::_Inner.train()\u001b[39m (pid=4175346, ip=10.1.65.9, actor_id=6f0bb06763ac8ade69f371bf01000000, repr=TorchTrainer)\n  File \"/opt/conda/lib/python3.13/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n    raise skipped from exception_cause(skipped)\n  File \"/opt/conda/lib/python3.13/site-packages/ray/train/_internal/utils.py\", line 57, in check_for_failure\n    ray.get(object_ref)\n    ~~~~~~~^^^^^^^^^^^^\n                                  ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=4178069, ip=10.1.65.9, actor_id=154138ab755d601f05ecd68a01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7fdad6762f90>)\n  File \"/opt/conda/lib/python3.13/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n    raise skipped from exception_cause(skipped)\n  File \"/opt/conda/lib/python3.13/site-packages/ray/train/_internal/utils.py\", line 176, in discard_return_wrapper\n    train_func(*args, **kwargs)\n    ~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_4165116/378395350.py\", line 221, in train_func\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mTrainingFailedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 365\u001b[39m\n\u001b[32m    362\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 361\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    354\u001b[39m trainer = TorchTrainer(\n\u001b[32m    355\u001b[39m     train_func,\n\u001b[32m    356\u001b[39m     scaling_config=scaling_config,\n\u001b[32m    357\u001b[39m     run_config=run_config,\n\u001b[32m    358\u001b[39m )\n\u001b[32m    360\u001b[39m \u001b[38;5;66;03m# Run the training\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# runs the function we passed to the trainer\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/ray/train/base_trainer.py:722\u001b[39m, in \u001b[36mBaseTrainer.fit\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    718\u001b[39m result = result_grid[\u001b[32m0\u001b[39m]\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.error:\n\u001b[32m    720\u001b[39m     \u001b[38;5;66;03m# Raise trainable errors to the user with a message to restore\u001b[39;00m\n\u001b[32m    721\u001b[39m     \u001b[38;5;66;03m# or configure `FailureConfig` in a new run.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m722\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m TrainingFailedError(\n\u001b[32m    723\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join([restore_msg, TrainingFailedError._FAILURE_CONFIG_MSG])\n\u001b[32m    724\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mresult\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merror\u001b[39;00m\n\u001b[32m    725\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mTrainingFailedError\u001b[39m: The Ray Train run failed. Please inspect the previous error messages for a cause. After fixing the issue (assuming that the error is not caused by your own application logic, but rather an error such as OOM), you can restart the run from scratch or continue this run.\nTo continue this run, you can use: `trainer = TorchTrainer.restore(\"/home/jovyan/ray_results/torch_trainer_example\")`.\nTo start a new run that will retry on training failures, set `train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for unlimited retries."
     ]
    }
   ],
   "source": [
    "# Import of all necessary functions and classes\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train import ScalingConfig\n",
    "from ray.air.config import RunConfig\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import torch.distributed as dist\n",
    "import ray\n",
    "import h5py\n",
    "import zarr as z\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../../src\")\n",
    "from juart.dl.loss.loss import JointLoss\n",
    "from juart.dl.operation.modules import training\n",
    "from juart.dl.utils.dist import GradientAccumulator\n",
    "from juart.dl.model.unrollnet import LookaheadModel, UnrolledNet\n",
    "from juart.dl.checkpoint.manager import CheckpointManager\n",
    "from juart.conopt.functional.fourier import (\n",
    "    fourier_transform_adjoint,\n",
    "    fourier_transform_forward,\n",
    "    nonuniform_fourier_transform_adjoint,\n",
    ")\n",
    "\n",
    "# activates the terminal output for print commands in ray\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Training function that is later passed to the TorchTrainer\n",
    "def train_func():\n",
    "\n",
    "    global_rank = int(dist.get_rank())\n",
    "    world_size = int(dist.get_world_size())\n",
    "    group_size = 2\n",
    "    \n",
    "    ################################################################\n",
    "    # Setting the rank for each worker\n",
    "    for rank in range(0, world_size, group_size):\n",
    "        ranks = list(range(rank, rank + group_size, 1))\n",
    "        device = f\"cuda:{global_rank}\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Rank {global_rank} is using devide {device} ...\")\n",
    "        if global_rank in ranks:\n",
    "            print(f\"Rank {global_rank} is in group {ranks} ...\")\n",
    "            group = dist.new_group(ranks, backend=\"gloo\")\n",
    "    \n",
    "    ################################################################\n",
    "    \n",
    "    # define variables\n",
    "    shape = (128,128,128,1,1)\n",
    "    nX, nY, nZ, nTI, nTE = shape\n",
    "    weight_kspace_loss = [0.5, 0.5] # weight the difference in k space\n",
    "    weight_ispace_loss = [0.0, 0.0] # weight the difference of the two images (dual domain) and average their gradients\n",
    "    weight_hankel_loss = [0.0, 0.0]\n",
    "    weight_casorati_loss = [0.0, 0.0]\n",
    "    weight_wavelet_loss = [0.0, 0.0] # weight the loss in wavelet domain\n",
    "    normalized_loss = True\n",
    "\n",
    "    batch_size = 1 # number of datapoints used per batch iteration\n",
    "    nD = 1 # number of datasets\n",
    "    nP = 2 # number of permutations per epoch\n",
    "    cgiter = 2 # number of dc iterations\n",
    "    num_epochs = 2 # number of epochs\n",
    "    \n",
    "    model_dir = f'nummodel{weight_ispace_loss[0]}i_{nP}P_{cgiter}DC_{num_epochs}E_R1'\n",
    "    root_dir =\"/home/jovyan/models\"\n",
    "    endpoint_url = \"https://s3.fz-juelich.de\"\n",
    "    model_backend = 'local'\n",
    "\n",
    "    single_epoch = False # if its true the script will stop after 1 epoch\n",
    "    save_checkpoint = True # enables checkpoint saving\n",
    "    checkpoint_frequency = 5 # number of iterations between the save files\n",
    "    load_model_state = True # if true the latest model state will be loaded if available\n",
    "    load_averaged_model_state = True # latest averaged model state will be loaded\n",
    "    load_optim_state = True # latest optimizer state will be loaded\n",
    "    load_metrics = True # the latest metrics (lost, iterations) will be loaded\n",
    "\n",
    "    num_groups = 1\n",
    "    batch_size_local = batch_size // num_groups\n",
    "    num_iterations = nD * nP * num_epochs\n",
    "    \n",
    "\n",
    "    # reading and shaping data\n",
    "    store = z.open(\"/home/jovyan/datasets/num_phantom_128_R1\")\n",
    "\n",
    "    C = torch.from_numpy(np.array(store[\"C\"])).to(device)\n",
    "    k = torch.from_numpy(np.array(store[\"k\"]))[...,None,None].to(device)\n",
    "    d = torch.from_numpy(np.array(store[\"d\"]))[...,None,None].to(device)\n",
    "\n",
    "    # --- shaping data ---\n",
    "    k_scaled = (k / (2 * k.max())).to(device)\n",
    "\n",
    "    generator = torch.Generator()\n",
    "\n",
    "    ################################################################\n",
    "    # Defining the neural network\n",
    "    \n",
    "    model = UnrolledNet(shape,\n",
    "                      CG_Iter = cgiter,\n",
    "                      num_unroll_blocks = 10,\n",
    "                      num_res_blocks = 15,\n",
    "                      features = 32,\n",
    "                      axes = (1,2,3),\n",
    "                      kernel_size = (3,3,3),\n",
    "                      activation = 'ReLU',\n",
    "                      ResNetCheckpoints = True).to(device)\n",
    "\n",
    "    loss_fn = JointLoss(\n",
    "        shape,\n",
    "        (3, 3),\n",
    "        weights_kspace_loss = weight_kspace_loss,\n",
    "        weights_ispace_loss = weight_ispace_loss,\n",
    "        weights_hankel_loss = weight_hankel_loss,\n",
    "        weights_casorati_loss = weight_casorati_loss,\n",
    "        weights_wavelet_loss = weight_wavelet_loss,\n",
    "        normalized_loss=normalized_loss,\n",
    "        group = group,\n",
    "        device=device,\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=0.0001,\n",
    "        betas=[0.9, 0.999],\n",
    "        eps=1.0e-8,\n",
    "        weight_decay=0.0,\n",
    "    )\n",
    "\n",
    "    accumulator = GradientAccumulator(\n",
    "        model,\n",
    "        accumulation_steps=batch_size_local,\n",
    "        max_norm=1.0,\n",
    "        normalized_gradient=False,\n",
    "    )\n",
    "\n",
    "    averaged_model = LookaheadModel(\n",
    "        model,\n",
    "        alpha=0.5,\n",
    "        k=5,\n",
    "    )\n",
    "\n",
    "    dist.barrier()\n",
    "    \n",
    "    checkpoint_manager = CheckpointManager(\n",
    "        model_dir,\n",
    "        root_dir=root_dir,\n",
    "        endpoint_url=endpoint_url,\n",
    "        backend=model_backend,\n",
    "    )\n",
    "\n",
    "    dist.barrier()\n",
    "\n",
    "    ################################################################\n",
    "    # LOADING CURRENT MODEL STATE\n",
    "    if load_model_state:\n",
    "        print(f\"Rank {global_rank} - Loading model state ...\")\n",
    "        checkpoint = checkpoint_manager.load([\"model_state\"], map_location=device)\n",
    "        if all(checkpoint.values()):\n",
    "            model.load_state_dict(checkpoint[\"model_state\"])\n",
    "        else:\n",
    "            print(f\"Rank {global_rank} - Could not load model state.\")\n",
    "    \n",
    "    if load_averaged_model_state:\n",
    "        print(f\"Rank {global_rank} - Loading averaged model state ...\")\n",
    "        checkpoint = checkpoint_manager.load(\n",
    "            [\"averaged_model_state\"], map_location=device\n",
    "        )\n",
    "        if all(checkpoint.values()):\n",
    "            averaged_model.load_state_dict(checkpoint[\"averaged_model_state\"])\n",
    "        else:\n",
    "            print(f\"Rank {global_rank} - Could not load averaged model state.\")\n",
    "    \n",
    "    if load_optim_state:\n",
    "        print(f\"Rank {global_rank} - Loading optim state ...\")\n",
    "        checkpoint = checkpoint_manager.load([\"optim_state\"], map_location=device)\n",
    "        if all(checkpoint.values()):\n",
    "            optimizer.load_state_dict(checkpoint[\"optim_state\"])\n",
    "        else:\n",
    "            print(f\"Rank {global_rank} - Could not load optim state.\")\n",
    "    \n",
    "        total_trn_loss = list()\n",
    "        total_val_loss = list()\n",
    "        iteration = 0\n",
    "    \n",
    "    if load_metrics:\n",
    "        print(f\"Rank {global_rank} - Loading metrics ...\")\n",
    "        checkpoint = checkpoint_manager.load([\"trn_loss\", \"val_loss\", \"iteration\"])\n",
    "        if all(checkpoint.values()):\n",
    "            total_trn_loss = list(checkpoint[\"trn_loss\"])\n",
    "            total_val_loss = list(checkpoint[\"val_loss\"])\n",
    "            iteration = checkpoint[\"iteration\"]\n",
    "        else:\n",
    "            print(f\"Rank {global_rank} - Could not load metrics.\")\n",
    "\n",
    "    print(f\"Rank {global_rank} - Continue with iteration {iteration} ...\")\n",
    "\n",
    "    dist.barrier()\n",
    "\n",
    "    ################################################################\n",
    "    # ACTUAL TRAINING LOOP\n",
    "    total_trn_loss = list()\n",
    "    total_val_loss = list()\n",
    "    iteration = 0\n",
    "\n",
    "    while iteration < num_iterations:\n",
    "        tic = time.time()\n",
    "        generator.manual_seed(iteration%nP)\n",
    "    \n",
    "        kspace_mask_worker0 = torch.randint(0, 2, (1, k_scaled.shape[1], 1, 1), generator=generator)\n",
    "        kspace_mask_worker1 = 1 - kspace_mask_worker0\n",
    "\n",
    "        # Defining data for worker 0\n",
    "        if global_rank == 0:\n",
    "            k_scaled_masked = (k_scaled * kspace_mask_worker0).to(device)\n",
    "            AHd = nonuniform_fourier_transform_adjoint(k_scaled_masked, d, (nX, nY, nZ)).to(device)\n",
    "            AHd = torch.sum(torch.conj(C[...,None,None]) * AHd, dim=0).to(device)\n",
    "        \n",
    "            data = [\n",
    "               {\n",
    "                   \"images_regridded\": AHd.to(device),\n",
    "                   \"kspace_trajectory\": k_scaled.to(device),\n",
    "                   \"sensitivity_maps\": C.to(device),\n",
    "                   \"kspace_mask_source\": kspace_mask_worker0.to(device),\n",
    "                   \"kspace_mask_target\": kspace_mask_worker1.to(device),\n",
    "                   \"kspace_data\": d.to(device),\n",
    "               }\n",
    "            ]\n",
    "\n",
    "        # Defining data for worker 1\n",
    "        elif global_rank == 1:\n",
    "            k_scaled_masked = (k_scaled * kspace_mask_worker1).to(device)\n",
    "            AHd = nonuniform_fourier_transform_adjoint(k_scaled_masked, d, (nX, nY, nZ)).to(device)\n",
    "            AHd = torch.sum(torch.conj(C[...,None,None]) * AHd, dim=0).to(device)\n",
    "        \n",
    "            data = [\n",
    "               {\n",
    "                   \"images_regridded\": AHd.to(device),\n",
    "                   \"kspace_trajectory\": k_scaled.to(device),\n",
    "                   \"sensitivity_maps\": C.to(device),\n",
    "                   \"kspace_mask_source\": kspace_mask_worker1.to(device),\n",
    "                   \"kspace_mask_target\": kspace_mask_worker0.to(device),\n",
    "                   \"kspace_data\": d.to(device),\n",
    "               }\n",
    "            ]\n",
    "    \n",
    "        trn_loss = training(\n",
    "           [0],\n",
    "           data,\n",
    "           model.to(device),\n",
    "           loss_fn.to(device),\n",
    "           optimizer.to(device),\n",
    "           accumulator.to(device),\n",
    "           group=group,\n",
    "           device=device,\n",
    "        )\n",
    "\n",
    "        val_loss = [0] * batch_size\n",
    "\n",
    "    ################################################################\n",
    "    # SAVING DATA\n",
    "        if global_rank == 0:\n",
    "            # Completed epoch\n",
    "            if (\n",
    "                save_checkpoint\n",
    "                and np.mod(iteration + batch_size, nD * nP) == 0\n",
    "            ):\n",
    "                print(\"Creating tagged checkpoint ...\")\n",
    "    \n",
    "                checkpoint = {\n",
    "                    \"iteration\": iteration + batch_size,\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"averaged_model_state\": averaged_model.state_dict(),\n",
    "                    \"optim_state\": optimizer.state_dict(),\n",
    "                    \"trn_loss\": total_trn_loss,\n",
    "                    \"val_loss\": total_val_loss,\n",
    "                }\n",
    "    \n",
    "                epoch = (iteration + batch_size) // (nD * nP)\n",
    "                checkpoint_manager.save(checkpoint, tag=f\"_epoch_{epoch}\")\n",
    "    \n",
    "                if single_epoch:\n",
    "                    # Also save the checkpoint as untagged checkpoint\n",
    "                    # Otherwise, training will be stuck in endless loop\n",
    "                    checkpoint_manager.save(checkpoint)\n",
    "                    checkpoint_manager.release()\n",
    "                    break\n",
    "    \n",
    "            # Intermediate checkpoint\n",
    "            elif (\n",
    "                save_checkpoint\n",
    "                and np.mod(iteration + batch_size, checkpoint_frequency) == 0\n",
    "            ):\n",
    "                print(\"Creating untagged checkpoint ...\")\n",
    "    \n",
    "                checkpoint = {\n",
    "                    \"iteration\": iteration + batch_size,\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"averaged_model_state\": averaged_model.state_dict(),\n",
    "                    \"optim_state\": optimizer.state_dict(),\n",
    "                    \"trn_loss\": total_trn_loss,\n",
    "                    \"val_loss\": total_val_loss,\n",
    "                }\n",
    "    \n",
    "                checkpoint_manager.save(checkpoint, block=False)\n",
    "    \n",
    "            toc = time.time() - tic\n",
    "    \n",
    "            print(\n",
    "                (\n",
    "                    f\"Iteration: {iteration} - \"\n",
    "                    + f\"Elapsed time: {toc:.0f} - \"\n",
    "                    + f\"Training loss: {[f'{loss:.3f}' for loss in trn_loss]} - \"\n",
    "                    + f\"Validation loss: {[f'{loss:.3f}' for loss in val_loss]}\"\n",
    "                )\n",
    "            )\n",
    "    \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "        iteration += batch_size\n",
    "\n",
    "    # Return the trained model\n",
    "    return {\"model\": model.parameters()}\n",
    "\n",
    "################################################################\n",
    "# main function that initializes needed classes and runs the train function\n",
    "def main():\n",
    "\n",
    "    dist.init_process_group(\n",
    "    backend=\"gloo\", init_method=\"tcp://127.0.0.1:23456\", world_size = 1, rank=0\n",
    "    )\n",
    "    \n",
    "    ray.init(runtime_env={\"working_dir\": \"/home/jovyan/juart/src\"})\n",
    "    scaling_config = ScalingConfig(\n",
    "        num_workers=2, # number of workers that should be initialized\n",
    "        use_gpu=True,  # should gpu be used?\n",
    "        resources_per_worker={\"CPU\": 24, \"GPU\": 1},\n",
    "    )\n",
    "\n",
    "    # Define the run configuration\n",
    "    run_config = RunConfig(\n",
    "        name=\"torch_trainer_example\", # name of the log file\n",
    "        verbose=1, # detail of the ouput\n",
    "    )\n",
    "\n",
    "    # Create the TorchTrainer\n",
    "    trainer = TorchTrainer(\n",
    "        train_func,\n",
    "        scaling_config=scaling_config,\n",
    "        run_config=run_config,\n",
    "    )\n",
    "\n",
    "    # Run the training\n",
    "    result = trainer.fit() # runs the function we passed to the trainer\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce0dc0-5bdf-442d-9e94-2a0ce82c5039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
